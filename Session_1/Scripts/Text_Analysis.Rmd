---
title: "Computational Text Analysis in R - A Primitive Introduction"
author: "Marius Saeltzer"
date: "2022-02-25"
output:
    ioslides_presentation:
    incremental: false
    widescreen: false 
    smaller: false 
---



```{r setup, include=FALSE}

#install.packages("knitr")
knitr::opts_chunk$set(echo = FALSE)

```

For this, we need 4 packages, including devtools, to download one from github.

```{r}
if(!require(udpipe)){install.packages('udpipe')}
if(!require(devtools)){install.packages("devtools")}
if(!require(quanteda)){install.packages("quanteda")}
devtools::install_github("kbenoit/quanteda.dictionaries") 


library(udpipe)
library(quanteda.dictionaries)
library(quanteda)
```

# The classicfier

As we start this course, we have not yet decided on what kind of data we want to work with. This depends on your research interests. For the first session, we will work with one of the most iconic datasets there is, the IMDB movie reviews dataset. It has 2 nice properties. It is labeled by the users (they give points for a movie) and it is used for the validation of all sorts of sentiment classifiers. 

The underlying concept is "sentiment" in the sense how much someone likes a particular movie. I got a sample of 5000 reviews here, if you want more, you find it not only in the textstat package, but in numerous other packages. A similar dataset is also in the package quanteda.corpora.

So let's start and check the data.

```{r}

data<-read.csv("../data/reviews.csv")
```

As we can see, we have a pretty balanced dataset 

```{r}

barplot(table(data$sentiment))

## how long is the text
hist(nchar(data$text))

```

## Primitive Dictionary

First, we want to see how base R functions for text analysis work. We use regular expressions here to simply match terms in the text. In this case, we just match the terms good and bad, and substract them. Primitive, stupid, but it will make a point.

```{r}


data$good<-grepl("good",data$text,ignore.case = T)

data$bad<-grepl("bad",data$text,ignore.case = T)

data$sent<-data$good-data$bad
  
```


First, we can see the main weakness of a dictionary. In most cases, the relevant terms don't actually occur, or cancel each other out. For half the dataset, we can't say anything, even though we know the posts are negative or positive. Ignoring them, we see slight tendencies in the right direction, but we can't see the correct answer. This is the 1) problem of natural language dictionaries: we use many terms for the same concept ("bad"). But it shows that a lot of reviews that are negative contain the word good, but not bad. This is the 2) problem of dictionaries: the use of the term in context ("Good Lord","Good Riddance"), use negations ("not good") or sarcasm. 

```{r}
table(data$sent,data$sentiment)

```







## quanteda

Ken Benoit produced a very useful package called quanteda, which makes text as data available in a fast, easy and powerful way. Before we start, a little terminology I will tell you more about later:

    - Corpus: A text "data set" which stores the raw text in the correct order as long strings - A string of words
    
    - Tokens: A List of words for each text, vectorized and therefore computable - A vector of words
    
    - DFM: A dataset that tells me for each word in which documents it occurs how often and for each document which words occur on in how often - a bag of words


## To Corpus

First we prepare the text and put it into a corpus:

Corpora are data objects to store large amounts of texts. R was not originally developed to store anything but numbers, a feature you will notice when you open large text stored in data.frame cells. It is very slow and highly inefficient. Regular expressions are among the slowest functions in R. 
Packages like snowball, tm, text2vec and most recent quanteda offer a corpus class that allows efficient word based operations. They are somewhat counterintuitive and resemble lists in the sense that they store meta data. Quanteda has the nice feature of docvars, putting text in a data.frame form, allowing a more data-like interpretation of corpora than tm.


A corpus is made up of texts and docvars. Docvars are document level variables like a data.frame

```{r,,message=FALSE,warning=F}

library(quanteda)

corp<-corpus(data$text,docvars 
           
           =data)

```

## Dictionary Analysis 


There are several dictionaries out there to perform sentiment analysis. One of the most popular in the social sciences, because it was developed for newspaper articles instead of movie reviews, is the lexicoder dictionary by Soroka et al.


To do dictionary analysis, we can use the Lexicoder dictionary implemented in quanteda.

```{r}

lsd<-quanteda::data_dictionary_LSD2015
lsd
```

The quanteda.dictionary package we downloaded from github contains a nice way to directly apply it to texts.



```{r} 

output_lsd <- liwcalike(corp, 
                       dictionary = lsd)
```

To classify the reviews, we will now score the individual reviews, based on the number of positive and negative features they have.

```{r}

data$score<-(output_lsd$positive+output_lsd$neg_negative)>(output_lsd$negative+output_lsd$neg_positive)
                              
t1<-table(data$score,data$sentiment)
sum(diag(t1)/sum(t1))

```
As we can see, the accuracy of this dictionary is not even that bad, it is substantially better than just guessing (an accuracy of 50% if we have so balanced classes).


But how can we do this in more detail? Dictionaries are the most simple form of rule-based classification. How can we plug them into more sophisticated methods? First, we want to focus on more rule-based approaches, namely grammar-based ways of preprocessing.


## Computer linguistic

First, we will take a look at what natural language processing can do: understanding the complexity of language.


To do this, we apply grammatical rules to the texts. We split them in sentences and words, find out the grammatical position and function, and offer the correct root forms. Basically, we do high school grammar, but automated, to be more concrete in what we look at. 

This can be very important, as many terms have different meaning depending on their position in a sentence.  

This is a very standardized task in linguitics, and due to the logic of the language and machine learning, we have well-working language models, as implemented in libraries like spacy in python, or udpipe in R. 

We download one of these language models:


```{r}

udpipe::udpipe_download_model("english")

``` 
...and import it.

```{r}
library(udpipe)
mod<-list.files()[grepl("udpipe",list.files())]
m1<-udpipe_load_model(mod)

```


Let's annotate n reviews, depending how much time you have ;)
```{r}
n<-50

```

using this function, we annotate the corpus
```{r}
annotations<-udpipe::udpipe_annotate(object = m1,data$text[1:n])
```


As you can see, it takes a while to only annotate only a small number of posts...

We then turn it into a data.frame to take a closer look.
```{r}
annotations_df<-as.data.frame(annotations)
```

We can now see how only 50 reviews get split in over 13766 words, which are annotated with their lemma, position in the sentence.


Let's look at some proper nouns:

```{r}

pn<-annotations_df[annotations_df$upos=="PROPN",]

head(pn$token,40)

```


So why should be do this? We can improve the performance of tasks further down the line, for example find out who is targeted by sentiment, or focus on specific types of words like adjectives. 

Let's apply the dictionary to adjectives only, for example. 

To do this, we need but get a bit creative and/or hackish. Because udpipe and quanteda are not easily compatible, we build our own method. 

Normally, you would just use the %in% operator to check for each term that occurs in the dictionary. But sadly, the Lexicoder is a number of regex expressions. So what to do? We will have to loop the regex over every token

For each token, we want to check if the terms in the Lexicoder match it.


```{r}
grepl(lsd$positive,r1$lemma)

```
doesn't work, because it will only check the first entry. So lets write a function that passes them all. It is incredibly inefficient, but does the job ;)

```{r}

check_dictionary<-function(text,dict){
  y<-F
  for(i in 1:length(dict)){
  y<-grepl(dict[i],text)
  if(y==T){break}
  }
  return(y)
}  
```


It also turned out that the dictionary used in quanteda has a typo...

To remove, do:
```{r}
lsd$negative<-gsub("\\*\\*","\\*",lsd$negative)
```

```{r}

### there is an error in the dictionary 

check_dictionary("horrendous",lsd$positive)
check_dictionary("horrendous",lsd$negative)
```

Now that we built our own dictionary function, we can apply it to the token level. Therefore, we can use the annotation function, to only check adjectives.


```{r}
r3<-annotations_df[annotations_df$upos=="ADJ",]

```

Next, we apply the function to the adjectives only...


```{r}
pos<-lapply(r3$token,function(x) check_dictionary(x,lsd$positive))


neg<-lapply(r3$token,function(x) check_dictionary(x,lsd$negative))

r3$pos<-unlist(pos)
r3$neg<-unlist(neg)

```

and 

```{r}
s_p<-aggregate(pos~r3$doc_id,data=r3,FUN="mean")
s_n<-aggregate(neg~r3$doc_id,data=r3,FUN="mean")
s<-cbind(s_p,s_n)
s$id<-as.numeric(gsub("doc","",s$`r3$doc_id`))
s<-s[order(s$id),]
s$senti<-data$sentiment[1:n]
s$lexi<-data$score[1:n]
s$lexi<-ifelse(s$lexi==T,"pos","neg")
s$lexi_adj<-ifelse(s$pos>s$neg,"pos","neg")

```
```{r}
t1<-table(s$lexi,s$senti)

t2<-table(s$lexi_adj,s$senti)

sum(diag(t1)/sum(t1))
sum(diag(t2)/sum(t2))

```

All in all, tiny bit better when only looking at adjectives, but this is a great lesson about accuracy metrics we will have later in this course...

Let's return to the slides for a second.





